<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Pei Zhou</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://shaoxia57.github.io" />
	    <meta property="og:title" content="Pei Zhou" />
	    <meta property="og:image" content="https://shaoxia57.github.io/img/pei_rainier.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Pei Zhou">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" crossorigin="anonymous">
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
    </head>
    <body>
        <div class="container mt-5">
            <div class="row mb-3">
                <div class="col-lg-3 col-md-4">
                    <img class="img-fluid rounded" src="img/pei_rainier.jpg" alt="Pei Zhou">
                    <p>
                        <i class="fa fa-envelope pt-3"></i> peiz@usc.edu
                    <!-- </br>
                        <i class="fa fa-building"></i> Gates 318 -->
                    </p>
                </div>
                <div class="col-lg-9 col-md-8">
                    <h1>Pei Zhou</h1>
                    <p>
                        I am a Senior Applied Scientist at Microsoft <a href="https://www.microsoft.com/en-us/research/group/office-of-applied-research" target="_blank">Office of Applied Research</a>,
			where I drive research on improving <b><a href="https://www.microsoft.com/en-us/microsoft-copilot/meet-copilot?ef_id=_k_de73ace12831147c6234ca1969a0d0c2_k_&OCID=AIDcmm9xzw3cn3_SEM__k_de73ace12831147c6234ca1969a0d0c2_k_&msclkid=de73ace12831147c6234ca1969a0d0c2" target="_blank">Copilot</a>
			reasoning for complex intent</b>, through <b>learning from interaction</b>, <b>synthetic data generation</b>, and <b>UX innovation</b>.
			
			I received my Ph.D. in Computer Science at
                        <a href="https://www.cs.usc.edu/" target="_blank">University of Southern California</a>, 
                        where I work in the <a href="https://nlp.usc.edu/"
                        target="_blank">USC-NLP Group</a>. 
                        My research interests are:
                        large language models (LLM) reasoning, communicating agents, and human-AI symbiosis.
                    </p>
                    <p>
                        Previously, I received undergraduate degree in Mathematics of Computation from University of
                        California, Los Angeles (UCLA). 
			I've also done research
                        at <a href="https://deepmind.google/"
                        target="_blank">Google Deepmind (Gemini)</a>, <a href="http://allenai.org/"
                                  target="_blank">Allen Institute for Artificial
                        Intelligence (AI2)</a>, and
                         <a href="https://www.amazon.science/" target="_blank">Amazon Alexa AI</a>. My work has been featured in The Register, Science Daily, VentureBeat, Tech Xplore, etc.
                    </p>
                    <p>
                        We are on lookout for exceptional applied science talents! If you have passion to improve LLM systems from interaction and excited about applied research impacts, feel free to drop me a line :)
                    </p>
                    <p>
                        [<a href="files/CV_dec23.pdf" target="_blank">Full CV</a>] [<a href="https://www.microsoft.com/en-us/research/people/zhoupei/" target="_blank">Microsoft Profile</a>]
			[<a href="https://twitter.com/peizNLP" target="_blank">Twitter</a>] [<a href="https://www.linkedin.com/in/pei-zhou-169051119" target="_blank">LinkedIn</a>] 
                        [<a href="https://github.com/shaoxia57" target="_blank">Github</a>] [<a href="https://scholar.google.com/citations?user=13PGDZsAAAAJ&hl=en" target="_blank">Google Scholar</a>] 
                        [<a href="https://www.semanticscholar.org/author/Pei-Zhou/1557324013" target="_blank">Semantic Scholar</a>]
                    </p>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                        <li>
                            [October 2024] Attending <a href = "https://neurips.cc/" 
                            target="_blank">COLM</a> in Penn and we are hiring interns and FTE, come say hi!
                        </li>
			<li>
                            [December 2023] Attending <a href = "https://neurips.cc/" 
                            target="_blank">NeurIPS</a> in New Orleans, come say hi!
                        </li>
                        <li>
                            [September 2023] Started my collaboration with <a href="https://deepmind.google/"
                            target="_blank">Google DeepMind</a> working on LLM meta-task reasoning!
                        </li>

                        <li>
                            [July 2023] Attending <a href = "https://2023.aclweb.org/" 
                            target="_blank">ACL</a> at Toronto, come say hi! I'll be presenting our 
                            <a href = "https://arxiv.org/pdf/2212.10060.pdf" target="_blank">paper</a> on a 
                            Dungeon Master-like dialogue agent in D&D with theory-of-mind and RL!
                        </li>
                        <li>
                            [May 2023] Started my internship at <a href="https://bard.google.com/"
                            target="_blank">Google Bard</a> working on theory-of-mind capabilities in LLMs!
                        </li>
                        <li>
                            [April 2023] Our <a href = "https://tomworkshop.github.io/" target="_blank">Theory-of-Mind workshop</a> 
                            is accepted in ICML 2023! Hope to see you in Honolulu in July!
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col">
                    <h2>Education</h2>
                    <p>
                        Aug 2019 - April 2024
                        <br>
                        Ph.D. in Computer Science, University of Southern California
                    </p>
                    <p>
                        Sep 2015 - June 2019
                        <br>
                        B.S. in Mathematics of Computation with Minor in Statistics, University of California, Los Angeles (UCLA)
                    </p>
                </div>
            </div>
            <hr>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Selected Publications </h2>
                    <p>(Full list see 
                        <a href="https://scholar.google.com/citations?user=13PGDZsAAAAJ&hl=en" target="_blank">Google Scholar</a>)</p>
                    <ul class="pl">
			<li>
                            <a href="https://arxiv.org/pdf/2310.03051.pdf" target="_blank">
                                <b>SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
				<a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>,
                        <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>,
			<a href="https://jungyhuk.github.io/" target="_blank"> Xinyun Chen</a>,
			<a href="https://research.google/people/heng-tzecheng/" target="_blank"> Heng-Tzi Cheng</a>,
			<a href="https://research.google/people/quocle/" target="_blank"> Quoc V. Le</a>,
			<a href="https://www.edchi.net/" target="_blank"> Ed H. Chi</a>,
			<a href="https://dennyzhou.github.io/" target="_blank"> Denny Zhou</a>,
                        <a href="https://swarooprm.github.io/"
                                target="_blank">Swaroop Mishra</a>, and <a href="https://scholar.google.com/citations?user=PyK4x4wAAAAJ"
                      target="_blank">Huaixiu Steven Zheng</a>.
                            <br/>
                            NeurIPS, 2024.
                            <br/>
                            <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#self_discover_abstract').toggle();return false;">abstract</a>]
			    [<a href="https://venturebeat.com/ai/google-deepmind-proposes-self-discover-framework-for-llms-improves-gpt-4-performance/" target="_blank">media coverage 1</a>]
				[<a href="https://techxplore.com/news/2024-02-discovery-approach-deepmind-framework-llms.html" target="_blank">media coverage 2</a>]
				[<a href=https://www.artificialintelligence-news.com/news/deepmind-framework-offers-breakthrough-llm-reasoning/" target="_blank">media coverage 3</a>]
                            <!-- [<a href="https://www.isi.edu/news/52601/choose-your-own-adventure-teaching-machines-to-think-before-they-speak/" target="_blank">media coverage</a>] -->
                            <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.poster.pdf" target="_blank">poster</a>]
                            [<a href="https://quoref-dataset.s3-us-west-2.amazonaws.com/train_and_dev/quoref-train-dev-v0.1.zip" target="_blank">dataset</a>]
                            [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="self_discover_abstract" class="abstract" style="display:none;">
                                <p>
                                    We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2’s performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.
                                </p>
                            </div>
                        </li>
                    </br>
                        <li>
                            <a href="https://arxiv.org/pdf/2310.03051.pdf" target="_blank">
                                <b>How FaR Are Large Language Models From Agents with Theory-of-Mind?</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://madaan.github.io/" target="_blank">Aman Madaan</a>,
                            <a href="https://www.linkedin.com/in/srividya-pranavi-potharaju" target="_blank">Srividya Pranavi Potharaju</a>,
                            <a href="https://www.linkedin.com/in/aditya2211/" target="_blank">Aditya Gupta</a>,
                            <a href="https://www.empiricallykev.com/"
                        target="_blank">Kevin R. McKee</a>,
                        <a href="https://ariholtzman.com/"
                        target="_blank">Ari Holtzman</a>,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>,
                        <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>,
                        <a href="https://swarooprm.github.io/"
                                target="_blank">Swaroop Mishra</a>
                            <a href="http://www.aidanematzadeh.me/" target="_blank">Aida Nematzadeh</a>,
                            <a href="http://shyamupa.com/"
                      target="_blank">Shyam Upadhyay</a>, and <a href="https://www.manaalfaruqui.com/"
                      target="_blank">Manaal Faruqui</a>.
                            <br/>
                            Preprint, 2023.
                            <br/>
                            <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#t4d_far_abstract').toggle();return false;">abstract</a>]
                            <!-- [<a href="https://www.isi.edu/news/52601/choose-your-own-adventure-teaching-machines-to-think-before-they-speak/" target="_blank">media coverage</a>] -->
                            <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.poster.pdf" target="_blank">poster</a>]
                            [<a href="https://quoref-dataset.s3-us-west-2.amazonaws.com/train_and_dev/quoref-train-dev-v0.1.zip" target="_blank">dataset</a>]
                            [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="t4d_far_abstract" class="abstract" style="display:none;">
                                <p>
                                    Thinking is for Doing. Humans can infer other people’s mental states from
                                    observations–an ability called Theory-of-Mind (ToM)–and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as
                                    ToMi ask models questions to make inferences about beliefs of characters in a story,
                                    but do not test whether models can then use these inferences to guide their actions.
                                    We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others’
                                    mental states to actions in social scenarios. Experiments on T4D demonstrate that
                                    LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters’ beliefs
                                    in stories, but they struggle to translate this capability into strategic action.

                                    Our analysis reveals the core challenge for LLMs lies in identifying the implicit
                                    inferences about mental states without being explicitly asked about as in ToMi,
                                    that lead to choosing the correct action in T4D. To bridge this gap, we introduce
                                    a zero-shot prompting framework, Foresee and Reflect (FaR), which provides
                                    a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4’s performance from 50% to 71%
                                    on T4D, outperforming other prompting methods such as Chain-of-Thought and
                                    Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures
                                    and scenarios that also require ToM inferences to choose an action, consistently
                                    outperforming other methods including few-shot in-context learning.
                                </p>
                            </div>
                        </li>
                    </br>
                    <li>
                        <a href="https://arxiv.org/pdf/2212.10465.pdf" target="_blank">
                            <b>SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization</b>
                        </a>
                        <br/>
                        <a href="https://hyunw.kim/" target="_blank">Hyunwoo Kim</a>,
                        <a href="https://jmhessel.com/" target="_blank">Jack Hessel</a>,
                        <a href="https://liweijiang.me/" target="_blank">Liwei Jiang</a>,
                        <a href="https://homes.cs.washington.edu/~pawest/"
                        target="_blank">Peter West</a>,
                        <a href="https://scholar.google.com/citations?user=ssYPSmkAAAAJ&hl=en"
                        target="_blank">Ximing Lu</a>,
                            <a href="https://yj-yu.github.io/home/"
                        target="_blank">Youngjae Yu</a>,
                        <b>Pei Zhou</b>,
                    <a href="https://rlebras.github.io/"
                    target="_blank">Ronan Le Bras</a>,
                    <a href="https://www.malihealikhani.com/"
                            target="_blank">Malihe Alikhani</a>
                        <a href="https://vision.snu.ac.kr/gunhee/" target="_blank">Gunhee Kim</a>,
                        <a href="https://maartensap.com/"
                  target="_blank">Maarten Sap</a>, and <a href="https://homes.cs.washington.edu/~yejin/" target="_blank">Yejin Choi</a>.
                        <br/>
                        In <a href = "https://2023.emnlp.org/" 
                        target="_blank"><b>EMNLP</b></a>, 2023.  <b>Outstanding Paper Award</b>
                        <br/>
                        <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.bib" target="_blank">bib</a>] -->
                        [<a href="#" onclick="$('#soda_abstract').toggle();return false;">abstract</a>]
                        <!-- [<a href="https://www.isi.edu/news/52601/choose-your-own-adventure-teaching-machines-to-think-before-they-speak/" target="_blank">media coverage</a>] -->
                        <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.poster.pdf" target="_blank">poster</a>]
                        [<a href="https://quoref-dataset.s3-us-west-2.amazonaws.com/train_and_dev/quoref-train-dev-v0.1.zip" target="_blank">dataset</a>]
                        [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                        <div id="soda_abstract" class="abstract" style="display:none;">
                            <p>
                                Data scarcity has been a long standing issue in
                                the field of open-domain social dialogue. To
                                quench this thirst, we present SODA: the first
                                publicly available, million-scale high-quality
                                social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from
                                a large language model. Human evaluation
                                shows that conversations in SODA are more
                                consistent, specific, and (surprisingly) natural
                                than those in prior human-authored datasets

                                Using SODA, we train COSMO: a generalizable conversation model that is significantly
                                more natural and consistent on unseen datasets
                                than best-performing conversation models (e.g.,
                                GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even
                                preferred to the original human-written gold
                                responses. Additionally, our results shed light
                                on the distinction between knowledge-enriched
                                conversations and natural social chitchats. We
                                make our data, models, and code public.
                            </p>
                        </div>
                    </li>
                </br>
                        <li>
                            <a href="https://arxiv.org/pdf/2212.10060.pdf" target="_blank">
                                <b>I Cast Detect Thoughts: Learning to Converse and Guide with Intents and
                                    Theory-of-Mind in Dungeons and Dragons</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://zhu.codes/" target="_blank">Andrew Zhu</a>,
                            <a href="https://jennhu.github.io/" target="_blank">Jennifer Hu</a>,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>,
                        <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>,
                            <a href="https://www.cis.upenn.edu/~ccb/" target="_blank">Chris Callison-Burch</a>,
                            <a href="https://homes.cs.washington.edu/~yejin/" target="_blank">Yejin Choi</a>,
                            and <a href="http://prithvirajva.com/"
                            target="_blank">Prithviraj Ammanabrolu</a>.
                            <br/>
                            In <a href = "https://2023.aclweb.org/" 
                            target="_blank"><b>ACL</b></a>, 2023.
                            <br/>
                            <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#dnd_acl2023_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://www.isi.edu/news/52601/choose-your-own-adventure-teaching-machines-to-think-before-they-speak/" target="_blank">media coverage</a>]
                            <!-- [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.poster.pdf" target="_blank">poster</a>]
                            [<a href="https://quoref-dataset.s3-us-west-2.amazonaws.com/train_and_dev/quoref-train-dev-v0.1.zip" target="_blank">dataset</a>]
                            [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="dnd_acl2023_abstract" class="abstract" style="display:none;">
                                <p>
                                    We propose a novel task, G4C, to study
                                    teacher-student natural language interactions
                                    in a goal-driven and grounded environment.
                                    Dungeons and Dragons (D&D), a role-playing
                                    game, provides an ideal setting to investigate
                                    such interactions. Here, the Dungeon Master
                                    (DM), i.e., the teacher, guides the actions of
                                    several players—students, each with their own
                                    personas and abilities—to achieve shared goals
                                    grounded in a fantasy world. Our approach is
                                    to decompose and model these interactions into
                                    (1) the DM’s intent to guide players towards
                                    a given goal; (2) the DM’s guidance utterance
                                    to the players expressing this intent; and (3) a
                                    theory-of-mind (ToM) model that anticipates
                                    the players’ reaction to the guidance one turn
                                    into the future. We develop a novel reinforcement learning (RL) method for training a DM
                                    that generates guidance for players by rewarding utterances where the intent matches the
                                    ToM-anticipated player actions. Human and
                                    automated evaluations show that a DM trained
                                    to explicitly model intents and incorporate ToM
                                    of the players using RL generates better-quality
                                    guidance that is 3x more likely to fulfill the
                                    DM’s intent than a vanilla natural language
                                    generation (NLG) approach.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/2211.09267.pdf" target="_blank">
                                <b>Reflect, Not Reflex: Inference-Based Common Ground Improves
                                    Dialogue Response Quality</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://justin-cho.com/" target="_blank">Hyundong Cho</a>,
                            <a href="https://www.linkedin.com/in/pegah-jandaghi-19a87489/" target="_blank">Pegah Jandaghi</a>,
                            <a href="https://www.danny-lee.info/" target="_blank">Dong-Ho Lee</a>,
                            <a href="https://yuchenlin.xyz/" target="_blank">Bill Yuchen Lin</a>,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>, and
                        <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>.
                            <br/>
                            In <a href="https://2022.emnlp.org/" target="_blank">
                                <b>EMNLP</b></a>, 2022.
                            <br/>
                            <!-- [<a href="papers/logan+liu+peters+gardner+singh.acl2019.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#reflect_emnlp2022_abstract').toggle();return false;">abstract</a>]
                            <!-- [<a href="papers/logan+liu+peters+gardner+singh.acl2019.poster.pdf" target="_blank">poster</a>] -->
                            [<a href="https://inklab.usc.edu/Reflect/" target="_blank">project page</a>]
                            [<a href="https://github.com/INK-USC/Reflect" target="_blank">dataset</a>]
                            [<a href="https://www.isi.edu/news/52601/choose-your-own-adventure-teaching-machines-to-think-before-they-speak/" target="_blank">media coverage</a>]
                            <div id="reflect_emnlp2022_abstract" class="abstract" style="display:none;">
                                <p>
                                    Human communication relies on common
                                    ground (CG), the mutual knowledge and beliefs shared by participants, to produce coherent and interesting conversations. In this paper, we demonstrate that current response generation (RG) models produce generic and dull
                                    responses in dialogues because they act reflexively, failing to explicitly model CG, both
                                    due to the lack of CG in training data and
                                    the standard RG training procedure. We introduce Reflect, a dataset that annotates dialogues
                                    with explicit CG (materialized as inferences
                                    approximating shared knowledge and beliefs)
                                    and solicits 9k diverse human-generated responses each following one common ground.
                                    Using Reflect, we showcase the limitations of
                                    current dialogue data and RG models: less
                                    than half of the responses in current data is
                                    rated as high quality (sensible, specific, and interesting) and models trained using this data
                                    have even lower quality, while most Reflect responses are judged high quality. Next, we analyze whether CG can help models produce better quality responses by using Reflect CG to
                                    guide RG models. Surprisingly, we find that
                                    simply prompting GPT3 to “think” about CG
                                    generates 30% more quality responses, showing promising benefits to integrating CG into
                                    the RG process.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/2110.08501.pdf" target="_blank">
                                <b>Think Before You Speak: Explicitly Generating Implicit Commonsense
                                    Knowledge for Response Generation</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://scholar.google.co.in/citations?hl=en&user=JD6DMWcAAAAJ&view_op=list_works" target="_blank">Karthik Gopalakrishnan</a>,
                            <a href="https://scholar.google.com/citations?user=MTG_OgQAAAAJ&hl=en" target="_blank">Behnam Hedayatnia</a>,
                            <a href="https://seokhwankim.github.io/" target="_blank">Seokhwan Kim</a>,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>,
                            <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>,
                            <a href="https://scholar.google.com/citations?user=w90wOucAAAAJ&hl=en"
                        target="_blank">Yang Liu</a>, and
                            <a href="https://scholar.google.com/citations?user=GMcL_9kAAAAJ&hl=en" target="_blank">Dilek Hakkani-Tur</a>.

                            <br/>
                            In <a href="https://www.2022.aclweb.org/" target="_blank">
                                <b>ACL</b></a>, 2022.
                            <br/>
                            <!-- [<a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#tbs_acl2022_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://viterbischool.usc.edu/news/2022/06/isi-researchers-train-artificial-intelligence-models-to-consider-common-sense-when-generating-responses/" target="_blank">media coverage</a>]
                            <!-- [slides:
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.pdf" target="_blank">pdf</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.with_speaker_notes.pdf" target="_blank">pdf with notes</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.key" target="_blank">key</a>]
                            [<a href="papers/contextual-repr-analysis" target="_blank">code</a>] -->
                            <div id="tbs_acl2022_abstract" class="abstract" style="display:none;">
                                <p>
                                    Implicit knowledge, such as common sense,
                                    is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly,
                                    omitting unstated implicit knowledge. In
                                    this paper, we present Think-Before-Speaking
                                    (TBS), a generative approach to first externalize implicit commonsense knowledge (think)
                                    and use this knowledge to generate responses
                                    (speak). We expect that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and
                                    enables more explainable models. We analyze different choices to collect knowledgealigned dialogues, represent implicit knowledge, and transition between knowledge and
                                    dialogues. Empirical results show TBS models outperform end-to-end and knowledgeaugmented RG baselines on most automatic
                                    metrics and generate more informative, specific, and commonsense-following responses,
                                    as evaluated by human annotators. TBS also
                                    generates knowledge that makes sense and is
                                    relevant to the dialogue around 85% of the
                                    time.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/2104.09574.pdf" target="_blank">
                                <b>Probing Commonsense Explanation in Dialogue Response Generation</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://www.linkedin.com/in/pegah-jandaghi-19a87489/" target="_blank">Pegah Jandaghi</a>,
                            <a href="https://justin-cho.com/" target="_blank">Hyundong Cho</a>,
                            <a href="https://yuchenlin.xyz/" target="_blank">Bill Yuchen Lin</a>,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>, and
                            <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>.
                            <br/>
                            In <a href="https://2021.emnlp.org/" target="_blank">
                                <b>EMNLP-Findings</b></a>, 2021.
                            <br/>
                            <!-- [<a href="papers/liu+schwartz+smith.naacl2019.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#cedar_emnlp2021_abstract').toggle();return false;">abstract</a>]
                            <!-- [slides:
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.pdf" target="_blank">pdf</a>,
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.with_speaker_notes.pdf" target="_blank">pdf with notes</a>,
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.key" target="_blank">key</a>] -->
                            <!-- [<a href="papers/inoculation-by-finetuning" target="_blank">code</a>] -->
                            <div id="cedar_emnlp2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Humans use commonsense reasoning (CSR)
                                    implicitly to produce natural and coherent responses in conversations. Aiming to close
                                    the gap between current response generation
                                    (RG) models and human communication abilities, we want to understand why RG models
                                    respond as they do by probing RG model’s
                                    understanding of commonsense reasoning that
                                    elicits proper responses. We formalize the
                                    problem by framing commonsense as a latent
                                    variable in the RG task and using explanations
                                    for responses as textual form of commonsense.
                                    We collect 6k annotated explanations justifying responses from four dialogue datasets and
                                    ask humans to verify them and propose two
                                    probing settings to evaluate RG models’ CSR
                                    capabilities. Probing results show that models fail to capture the logical relations between
                                    commonsense explanations and responses and
                                    fine-tuning on in-domain data and increasing
                                    model sizes do not lead to understanding of
                                    CSR for RG. We hope our study motivates
                                    more research in making RG models emulate the human reasoning process in pursuit of
                                    smooth human-AI communication.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/2103.11320.pdf" target="_blank">
                                <b>Lawyers are Dishonest? Quantifying Representational Harms in
                                    Commonsense Knowledge Resources</b>
                            </a>
                            <br/>
                            <a href="https://ninarehm.github.io/" target="_blank">Ninareh Mehrabi*</a>,
                            <b>Pei Zhou* (equal contribution)</b>,
                            <a href="https://www.isi.edu/~fredmors/" target="_blank">Fred Morstatter</a>,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>,
                            <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>,
                            and <a href="https://www.isi.edu/people/galstyan/about" target="_blank">Aram Galstyan</a>.
                            <br/>
                            In <a href="https://2021.emnlp.org/" target="_blank">
                                <b>EMNLP</b></a>, 2021.
                            <br/>
                            <!-- [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#bias_emnlp2021_abstract').toggle();return false;">abstract</a>]
                            <!-- [<a href="https://huggingface.co/datasets/viewer/?dataset=common_gen" target="_blank">media coverage</a>] -->
                            <!-- [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.slides.pdf" target="_blank">(short) slides</a>]
                            [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.poster.pdf" target="_blank">poster</a>]
                            [<a href="papers/lstms-exploit-linguistic-attributes" target="_blank">code</a>] -->
                            <div id="bias_emnlp2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Numerous natural language processing models have tried injecting commonsense by using the ConceptNet knowledge base to improve performance on different tasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect human biases such as "lawyers are dishonest." It is important that these biases are not conflated with the notion of commonsense. We study this missing yet important problem by first defining and quantifying biases in ConceptNet as two types of representational harms: overgeneralization of polarized perceptions and representation disparity. We find that ConceptNet contains severe biases and disparities across four demographic categories. In addition, we analyze two downstream models that use ConceptNet as a source for commonsense knowledge and find the existence of biases in those models as well. We further propose a filtered-based bias-mitigation approach and examine its effectiveness. We show that our mitigation approach can reduce the issues in both resource and models but leads to a performance drop, leaving room for future work to build fairer and stronger commonsense models.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/2005.00782.pdf" target="_blank">
                                <b>RICA: Evaluating Robust Inference Capabilities
                                    Based on Commonsense Axioms</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://www.linkedin.com/in/rahul-khanna-1b97317a/" target="_blank">Rahul Khanna</a>,
                            Seyeon Lee,
                            <a href="https://yuchenlin.xyz/" target="_blank">Bill Yuchen Lin</a>,
                            Daniel Ho,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>, and
                            <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>,

                            <br/>
                            In <a href="https://2021.emnlp.org/" target="_blank">
                                <b>EMNLP</b></a>, 2021.
                            <br/>
                            <!-- [<a href="papers/allennlp.nlposs2018.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#rica_emnlp2021_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://inklab.usc.edu/RICA/" target="_blank">project page</a>]
                            [<a href="https://github.com/shaoxia57/RICA" target="_blank">data</a>]
                            <div id="rica_emnlp2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Pre-trained language models (PTLMs) have
                                    achieved impressive performance on commonsense inference benchmarks, but their ability
                                    to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit
                                    of advancing fluid human-AI communication,
                                    we propose a new challenge, RICA: Robust
                                    Inference using Commonsense Axioms, that
                                    evaluates robust commonsense inference despite textual perturbations. To generate data
                                    for this challenge, we develop a systematic and
                                    scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more
                                    than 10k statements show that PTLMs perform no better than random guessing on the
                                    zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning
                                    on similar statements offer limited gains, as
                                    PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and
                                    human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/2109.06427.pdf" target="_blank">
                                <b>Commonsense-Focused Dialogues for Response Generation: An Empirical Study</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://scholar.google.co.in/citations?hl=en&user=JD6DMWcAAAAJ&view_op=list_works" target="_blank">Karthik Gopalakrishnan</a>,
                            <a href="https://scholar.google.com/citations?user=MTG_OgQAAAAJ&hl=en" target="_blank">Behnam Hedayatnia</a>,
                            <a href="https://seokhwankim.github.io/" target="_blank">Seokhwan Kim</a>,
                            <a href="https://www.jaypujara.org/"
                        target="_blank">Jay Pujara</a>,
                            <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>,
                            <a href="https://scholar.google.com/citations?user=w90wOucAAAAJ&hl=en"
                        target="_blank">Yang Liu</a>, and
                            <a href="https://scholar.google.com/citations?user=GMcL_9kAAAAJ&hl=en" target="_blank">Dilek Hakkani-Tur</a>.

                            <br/>
                            In <a href="https://www.2022.aclweb.org/" target="_blank">
                                <b>SIGDIAL</b></a>, 2021.
                            <br/>
                            <!-- [<a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#csdialog_sigdial2021_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/alexa/Commonsense-Dialogues" target="_blank">data</a>]
                            [<a href="https://www.amazon.science/blog/amazon-releases-new-dataset-for-commonsense-dialogue" target="_blank">Amazon blog</a>]
                            <!-- [slides:
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.pdf" target="_blank">pdf</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.with_speaker_notes.pdf" target="_blank">pdf with notes</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.key" target="_blank">key</a>]
                            [<a href="papers/contextual-repr-analysis" target="_blank">code</a>] -->
                            <div id="csdialog_sigdial2021_abstract" class="abstract" style="display:none;">
                                <p>
                                    Smooth and effective communication requires
                                    the ability to perform latent or explicit commonsense inference. Prior commonsense reasoning benchmarks (such as SocialIQA and
                                    CommonsenseQA) mainly focus on the discriminative task of choosing the right answer
                                    from a set of candidates, and do not involve
                                    interactive language generation as in dialogue.
                                    Moreover, existing dialogue datasets do not
                                    explicitly focus on exhibiting commonsense
                                    as a facet. In this paper, we present an empirical study of commonsense in dialogue response generation. We first auto-extract commonsensical dialogues from existing dialogue
                                    datasets by leveraging ConceptNet, a commonsense knowledge graph. Furthermore, building on social contexts/situations in SocialIQA,
                                    we collect a new dialogue dataset with 25K
                                    dialogues aimed at exhibiting social commonsense in an interactive setting. We evaluate response generation models trained using these
                                    datasets and find that models trained on both
                                    extracted and our collected data produce responses that consistently exhibit more commonsense than baselines. Finally we propose an approach for automatic evaluation of
                                    commonsense that relies on features derived
                                    from ConceptNet and pretrained language and
                                    dialog models, and show reasonable correlation with human evaluation of responses’
                                    commonsense quality. We are releasing a
                                    subset of our collected data, CommonsenseDialogues1
                                    , containing about 11K dialogs.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/1911.03705.pdf" target="_blank">
                                <b>CommonGen: A Constrained Text Generation Challenge
                                    for Generative Commonsense Reasoning</b>
                            </a>
                            <br/>
                            <a href="https://yuchenlin.xyz/" target="_blank">Bill Yuchen Lin</a>,
                            <a href="https://michaelzhouwang.github.io/" target="_blank">Wangchunshu Zhou</a>,
                            Ming Shen,
                            <b>Pei Zhou</b>,
                            <a href="https://www.chandrab.page/" target="_blank">Chandra Bhagavatula</a>,
                            <a href="https://homes.cs.washington.edu/~yejin/"
                        target="_blank">Yejin Choi</a>, and
                            <a href="https://shanzhenren.github.io/"
                        target="_blank">Xiang Ren</a>.

                            <br/>
                            In <a href="https://2020.emnlp.org/" target="_blank">
                                <b>EMNLP-Findings</b></a>, 2020.
                            <br/>
                            <!-- [<a href="papers/liu+levow+smith.sclem2018.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#commongen_emnlp2020_abstract').toggle();return false;">abstract</a>]
                            <!-- [<a href="papers/liu+levow+smith.sclem2018.poster.pdf" target="_blank">poster</a>] -->
                            [<a href="https://inklab.usc.edu/CommonGen/" target="_blank">project page</a>]
                            [<a href="https://huggingface.co/datasets/viewer/?dataset=common_gen" target="_blank">data</a>]
                            [<a href="https://huggingface.co/datasets/viewer/?dataset=common_gen" target="_blank">media coverage</a>]
                            <div id="commongen_emnlp2020_abstract" class="abstract" style="display:none;">
                                <p>
                                    Recently, large-scale pretrained language models have demonstrated impressive performance
                                    on several commonsense-reasoning benchmark datasets. However, building machines
                                    with commonsense to compose realistically
                                    plausible sentences remains challenging. In
                                    this paper, we present a constrained text generation task, COMMONGEN associated with a
                                    benchmark dataset, to explicitly test machines
                                    for the ability of generative commonsense reasoning. Given a set of common concepts (e.g.,
                                    {dog, frisbee, catch, throw}); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., “a man
                                    throws a frisbee and his dog catches it”).
                                    The COMMONGEN task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations.
                                    Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between
                                    state-of-the-art text generation models (e.g.,
                                    T5) and human performance. Furthermore, we
                                    demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks by generating additional context.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/pdf/1909.02224.pdf" target="_blank">
                                <b>Examining Gender Bias in Languages with Grammatical Gender</b>
                            </a>
                            <br/>
                            <b>Pei Zhou</b>,
                            <a href="https://weijia-shi.netlify.app/" target="_blank">Weijia Shi</a>,
                            <a href="https://jyzhao.net/" target="_blank">Jieyu Zhao</a>,
                            <a href="https://khhuang.me/" target="_blank">Kuan-Hao Huang</a>,
                            <a href="https://muhaochen.github.io/" target="_blank">Muhao Chen</a>,
                            <a href="https://rycolab.io/authors/ryan/" target="_blank">Ryan Cotterell</a>,
                            and <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang</a>.
                            <br/>
                            In <a href="https://2019.emnlp.org/" target="_blank">
                                <b>EMNLP-IJCNLP</b></a>, 2019.
                            <br/>
                            <!-- [<a href="papers/welbl+liu+gardner.wnut2017.bib" target="_blank">bib</a>] -->
                            [<a href="#" onclick="$('#bias_embedding_emnlp2019_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/shaoxia57/Bias_in_Gendered_Languages" target="_blank">code</a>]
                            <!-- [<a href="papers/sciq/welbl+liu+gardner.wnut2017.poster.pdf" target="_blank">poster</a>] -->
                            <div id="bias_embedding_emnlp2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Recent studies have shown that word embeddings exhibit gender bias inherited from the
                                    training corpora. However, most studies to
                                    date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages
                                    that exhibit morphological agreement on gender, such as Spanish and French. In this paper,
                                    we propose new metrics for evaluating gender
                                    bias in word embeddings of these languages
                                    and further demonstrate evidence of gender
                                    bias in bilingual embeddings which align these
                                    languages with English. Finally, we extend an
                                    existing approach to mitigate gender bias in
                                    word embeddings under both monolingual and
                                    bilingual settings. Experiments on modified
                                    Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches
                                    effectively reduce the gender bias while preserving the utility of the embeddings.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            
            <div class="row">
                <div class="col">
                    <h2>Internships</h2>
                    <ul>
                        <li>
                            Research Intern@<a href="https://bard.google.com/"
                            target="_blank">Google Bard</a>, Mountain View, CA, May-Aug 2023
                        </li>
                        <li>
                            Research Intern@<a href="http://allenai.org/"
                            target="_blank">AI2-Mosaic</a>, Seattle, WA, May-Dec 2022
                        </li>
                        <li>
                            Applied Scientist Intern@<a href="https://www.amazon.science/"
                            target="_blank">Amazon Alexa AI</a>, Remote, May-Aug 2021
                        </li>
                        <li>
                            Applied Scientist Intern@<a href="https://www.amazon.science/"
                            target="_blank">Amazon Alexa AI</a>, Remote, May-Aug 2020
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            
            <div class="row">
                <div class="col">
                    <h2>Miscellany</h2>
                    <ul>
                        <li>
                            I play the piano and am a keyboardist in UCLA Accoustic Guitar Band called Parked in 4 East
                        </li>
                        <li>
                            I was born in Chengdu, a great city for vacation and spicy food lovers :)
                        </li>
                        <li>
                            Currently super into camping/glamping!
                        </li>
                        <li>
                            I'm into all kinds of RPGs from tabletop to ffxiv
                        </li>
                    
                        <li>
                            Thanks to <a href="https://nelsonliu.me/"
                            target="_blank">Nelson Liu</a> for sharing source code of the website!
                        </li>
                    </ul>
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Pei Zhou, 2023
                        </p>
                    </div>
                    <!-- <div class="col-6 col-md text-right">
                        <a href="https://nlp.stanford.edu" class="image-link">
                            <img class="mr-4" src="img/stanford_nlp_logo.gif" alt="Stanford NLP Group logo." height="75">
                        </a>
                        <a href="http://ai.stanford.edu" class="image-link">
                            <img src="img/stanford_ai_logo.jpg" alt="Stanford AI Lab logo." height="75">
                        </a>
                    </div> -->
                </div>
            </footer>
        </div>
    </body>
</html>
